{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Good anon, took the NAI files apart and put together a working web generator that works exactly like the original. \n",
        "\n",
        "I adapted it into a collab"
      ],
      "metadata": {
        "id": "CX9IagGrYBzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Colab was created by user: daswer123**\n",
        "\n",
        "https://github.com/daswer123/stable-diffusion-colab\n",
        "\n",
        "**If you liked the colab, give it a star :)**"
      ],
      "metadata": {
        "id": "iJrL4EKeAbKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "bPyY7QA7Ud68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Func to change model Path\n",
        "def replace_line(file_name, line_num, text):\n",
        "    lines = open(file_name, 'r').readlines()\n",
        "    lines[line_num] = text\n",
        "    out = open(file_name, 'w')\n",
        "    out.writelines(lines)\n",
        "    out.close()\n",
        "\n",
        "def replace_line_in_file(file_name, line_to_search, text_to_replace):\n",
        "    with open(file_name, 'r') as file:\n",
        "        # read a list of lines into data\n",
        "        data = file.readlines()\n",
        "\n",
        "    for line in data:\n",
        "        # if the line contains the string we're looking for,\n",
        "        # write the line to the output file\n",
        "        if line_to_search in line:\n",
        "            replace_line(file_name, data.index(line), text_to_replace)\n",
        "\n",
        "# !pip install virtualenv\n",
        "!python --version\n",
        "!pip install uvicorn\n",
        "!npm install -g localtunnel\n",
        "!pip install gdown\n",
        "!gdown https://huggingface.co/Daswer123/asdasdadsa/resolve/main/naifu.zip #pls don't like it\n",
        "!unzip naifu.zip\n",
        "!mv models /notebooks/program/\n",
        "!rm -rf /notebooks/naifu.zip #Remove unnecessary files"
      ],
      "metadata": {
        "id": "HfD0Lugl-9De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /notebooks/program\n",
        "!pip install -r /notebooks/program/requirements.txt\n",
        "!wget https://pastebin.com/raw/fBYfa6hJ -O run.sh\n",
        "\n",
        "replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/animefull-final-pruned\"\\n')\n",
        "\n",
        "sfw_dowload = False\n",
        "costom_dowload = False"
      ],
      "metadata": {
        "id": "nUoBhgaAC5Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for determining compute capability \n",
        "def get_compute_capability(string):\n",
        "    return string.split(',')[3].split(':')[1].strip()\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "gpu = device_lib.list_local_devices()\n",
        "compute = get_compute_capability(gpu[1].physical_device_desc)\n",
        "\n",
        "print(gpu[1].physical_device_desc)\n",
        "print(compute)\n",
        "\n",
        "#Install xformers \n",
        "# %cd /notebooks/stable-diffusion-webui/repositories/\n",
        "# !git clone https://github.com/facebookresearch/xformers.git\n",
        "# %cd xformers\n",
        "# !git submodule update --init --recursive\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# %cd /notebooks/stable-diffusion-webui\n",
        "\n",
        "#Install precompiled wheel\n",
        "if(compute == \"8.6\"):\n",
        "    !pip install https://github.com/daswer123/stable-diffusion-colab/raw/main/xformers%20prebuild/sm_86/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\n",
        "if(compute == \"7.5\"):\n",
        "    !pip install https://github.com/daswer123/stable-diffusion-colab/raw/main/xformers%20prebuild/sm_75/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\n",
        "if(compute == \"6.1\"):\n",
        "    !pip install https://github.com/daswer123/stable-diffusion-colab/raw/main/xformers%20prebuild/sm_61/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\n",
        "\n",
        "\n",
        "#Need for xformers\n",
        "%cd /notebooks/program\n",
        "!git clone https://github.com/openai/triton.git\n",
        "%cd triton/python\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "f0yVgKl5jyBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select **full** or **sfw** model or you can upload your model\n",
        "\n",
        "## you can reselect without reloading\n",
        "The default setting is full\n",
        "\n",
        "Select custom only when you load your own model"
      ],
      "metadata": {
        "id": "JyDR5UEGpugB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full \n",
        "# sfw\n",
        "# custom \n",
        "Model = \"full\""
      ],
      "metadata": {
        "id": "YqpPrMMSHeCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "load_model_via_hugginface = False # Change to False if you want to load model via hugginface\n",
        "\n",
        "#Insert a link like this\n",
        "#https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\n",
        "\n",
        "hugginface_url = \"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "#Working code\n",
        "if (load_model_via_hugginface):\n",
        "  %cd /notebooks/program/models\n",
        "  !mkdir custom\n",
        "  %cd custom\n",
        "\n",
        "  print(\"dowload model and moving it to custom folder\")\n",
        "  !gdown $hugginface_url -O model.ckpt\n",
        "  !gdown https://huggingface.co/Daswer123/asdasdadsa/resolve/main/sfw.yaml -O config.yaml\n",
        "  custom_dowload = True\n",
        "\n",
        "if (Model == \"sfw\" and sfw_dowload == False):\n",
        "  %cd /notebooks/program/models\n",
        "  !mkdir sfw\n",
        "  %cd sfw \n",
        "  !gdown https://huggingface.co/Daswer123/asdasdadsa/resolve/main/sfw.ckpt -O model.ckpt\n",
        "  !gdown https://huggingface.co/Daswer123/asdasdadsa/resolve/main/sfw.yaml -O config.yaml\n",
        "  sfw_dowload = True;\n",
        "\n",
        "\n",
        "\n",
        "%cd /notebooks/program\n",
        "\n",
        "if (Model == \"sfw\"):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/sfw\"\\n')\n",
        "  print(\"Your model is: SFW\")\n",
        "elif (Model == \"full\"):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/animefull-final-pruned\"\\n')\n",
        "  print(\"Your model is: FULL\")\n",
        "elif (Model == \"custom\" and custom_dowload):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/custom\"\\n')\n",
        "  print(\"Your model is: CUSTOM\")\n",
        "else:\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"animefull-final-pruned\"\\n')\n",
        "  print(\"Your model must be loaded via gdrive before it can be run\")\n",
        "  print(\"Your model is: FULL\")"
      ],
      "metadata": {
        "id": "NBrmRspaJ8Er",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run cell below and follow command\n",
        "\n",
        "1) Open terminal\n",
        "\n",
        "2) npm i localtunnel\n",
        "\n",
        "3) lt -l 0.0.0.0 -p 6969\n",
        "\n",
        "You get the link, wait for the model to load and follow the link from the terminal"
      ],
      "metadata": {
        "id": "3QdNUhb5GwUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /notebooks/program\n",
        "!bash run.sh"
      ],
      "metadata": {
        "id": "DZuxSM1emeTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "1) Keep in mind that in NAI Full \"bad anatomy + low quality\" and \"low quality\" is the same as \"NSFW + bad anatomy + low quality\" and \"NSFW + low quality\" \n",
        "\n",
        "But NAI Sfw uses the same options as the original generator\n",
        "\n",
        "2) 99% of the time the result is identical to NAI Full and Sfw in their website\n",
        "\n",
        "3) Once you have loaded the model with hugginface or gdrive you can, uncheck the checkbox and just switch the model without loading again"
      ],
      "metadata": {
        "id": "wmSOGu5hoEbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RESTART COLAB, IF YOU GET SOME TROUBLE\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "IW5bEytDtxfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}